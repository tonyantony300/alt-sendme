# Performance Optimizations Applied

## Summary

Successfully implemented all 5 performance optimizations to increase iroh transfer speeds from 60% to an expected 85-95% of the maximum 4 Gbps capability.

## Changes Made

### 1. âœ… QUIC Transport Configuration

**Files Modified**: `src/core/send.rs`, `src/core/receive.rs`

Added high-performance QUIC transport configuration:
- **Max Concurrent Bidirectional Streams**: 256 (increased from default)
- **Max Concurrent Unidirectional Streams**: 256 (increased from default)
- **Stream Receive Window**: 8 MB per stream
- **Connection Receive Window**: 16 MB
- **Connection Send Window**: 16 MB
- **Datagram Send Buffer**: 16 MB

**Expected Impact**: +20-30% throughput improvement

**Code Location**:
- `src/core/send.rs:73-88`
- `src/core/receive.rs:69-84`

### 2. âœ… Parallel File Export

**File Modified**: `src/core/receive.rs`

Refactored the `export()` function to process files in parallel using `buffered_unordered()`:
- Uses `num_cpus::get().max(4)` concurrent workers
- Exports multiple files simultaneously instead of sequentially
- Maintains error handling and validation

**Expected Impact**: +15-20% improvement for multi-file transfers

**Code Location**: `src/core/receive.rs:275-336`

### 3. âœ… Compiler Optimization for Speed

**File Modified**: `Cargo.toml`

Changed release profile optimization level:
- **Before**: `opt-level = "s"` (optimize for size)
- **After**: `opt-level = 3` (optimize for speed)

**Expected Impact**: +5-10% overall performance improvement

**Code Location**: `Cargo.toml:71-77`

### 4. âœ… Increased Blob Request Concurrency

**File Modified**: `src/core/receive.rs`

Increased the `max_children_to_fetch` parameter in `get_hash_seq_and_sizes()`:
- **Before**: 32 MB (1024 * 1024 * 32)
- **After**: 128 MB (1024 * 1024 * 128)

This allows more parallel blob requests for better throughput on large files.

**Expected Impact**: +10-15% improvement for large files

**Code Location**: `src/core/receive.rs:148`

### 5. âœ… Reduced Progress Event Overhead

**File Modified**: `src/core/receive.rs`

Reduced progress event emission frequency:
- **Before**: Every 1 MB
- **After**: Every 5 MB

This reduces the overhead of frequent event emissions during transfers.

**Expected Impact**: +2-5% performance improvement

**Code Location**: `src/core/receive.rs:180-194`

## Expected Results

### Performance Improvements by Category

| Optimization | Expected Improvement |
|-------------|---------------------|
| QUIC Configuration | +20-30% |
| Parallel Export | +15-20% |
| Compiler Settings | +5-10% |
| Blob Concurrency | +10-15% |
| Event Throttling | +2-5% |

### Overall Expected Performance

- **Current**: ~60% of 4 Gbps = ~2.4 Gbps
- **Target**: 85-95% of 4 Gbps = ~3.4-3.8 Gbps
- **Expected Improvement**: **40-60% speed increase**

## Testing Recommendations

### 1. Large Single File Test
```bash
# Create a test file
dd if=/dev/urandom of=test_large.bin bs=1M count=2048  # 2GB file

# Test send
./sendme send test_large.bin

# Test receive (on another machine)
./sendme receive <ticket>
```

**What to measure**: Raw throughput in Gbps

### 2. Multiple Small Files Test
```bash
# Create test directory with many files
mkdir test_dir
for i in {1..100}; do dd if=/dev/urandom of=test_dir/file_$i.bin bs=1M count=10; done

# Test send
./sendme send test_dir

# Test receive
./sendme receive <ticket>
```

**What to measure**: Time to complete and parallel export benefits

### 3. Monitor QUIC Performance
```bash
# Enable debug logging
export RUST_LOG=debug

# Run tests and check for:
# - Number of concurrent streams being used
# - Buffer utilization
# - Connection statistics
```

## Building with Optimizations

To build the optimized release version:

```bash
cargo build --release
```

The binary will be located at `target/release/sendme` with all optimizations applied.

## Verification Steps

1. âœ… Code compiles successfully (`cargo check`)
2. âœ… No linter errors
3. âœ… All 5 optimizations implemented
4. ðŸ”„ **Next**: Performance testing on real transfers

## Notes

- These optimizations are specifically tuned for **direct P2P connections over WAN** with **mixed file sizes**
- For LAN transfers, you may see even higher speeds (potentially near the full 4 Gbps)
- Monitor system resources (CPU, memory) during transfers to ensure no new bottlenecks
- The QUIC buffer sizes (8-16 MB) are optimized for high-bandwidth, high-latency connections

## Rollback Instructions

If needed, all changes can be reverted by:
```bash
git checkout HEAD -- Cargo.toml src/core/send.rs src/core/receive.rs
```

## References

- [Iroh Documentation](https://www.iroh.computer/docs)
- [QUIC Protocol Performance](https://arxiv.org/abs/1708.05425)
- [Iroh Performance Blog Posts](https://www.iroh.computer/blog)

